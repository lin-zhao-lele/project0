"""
LSTMè‚¡ç¥¨ä»·æ ¼é¢„æµ‹æ¨¡åž‹ - é¢„æµ‹è„šæœ¬

æœ¬è„šæœ¬å®žçŽ°ä»¥ä¸‹åŠŸèƒ½ï¼š
1. åŠ è½½è®­ç»ƒå¥½çš„LSTMæ¨¡åž‹
2. åŠ è½½å¹¶é¢„å¤„ç†é¢„æµ‹æ•°æ®
3. ä½¿ç”¨æ»‘åŠ¨çª—å£æŠ€æœ¯è¿›è¡Œé¢„æµ‹
4. åå½’ä¸€åŒ–é¢„æµ‹ç»“æžœ
5. ä¿å­˜é¢„æµ‹ç»“æžœå’Œæ€§èƒ½æŒ‡æ ‡

é€‚åˆåˆå­¦è€…å­¦ä¹ LSTMæ¨¡åž‹çš„é¢„æµ‹æµç¨‹
"""
import os
import torch
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler  # æ•°æ®å½’ä¸€åŒ–
import json
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from module.visualization.pltTrend import plot_trend_signals_from_csv

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(os.path.dirname(BASE_DIR))
DATA_DIR = os.path.join(os.path.join(PROJECT_ROOT, "data"), "raw")
RESULTS_DIR = os.path.join(BASE_DIR, "results")

# Device configuration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

"""
LSTMæ¨¡åž‹å®šä¹‰(ä¸Žtrain_model.pyä¸­ç›¸åŒ)

è¿™æ˜¯ä¸€ä¸ªæ ‡å‡†çš„LSTMç½‘ç»œç»“æž„ï¼ŒåŒ…å«ä»¥ä¸‹ç»„ä»¶ï¼š
1. LSTMå±‚ï¼šå¤„ç†æ—¶åºæ•°æ®
2. å…¨è¿žæŽ¥å±‚ï¼šå°†LSTMè¾“å‡ºæ˜ å°„åˆ°é¢„æµ‹å€¼

å‚æ•°è¯´æ˜Žï¼š
- input_size: è¾“å…¥ç‰¹å¾ç»´åº¦
- hidden_size: LSTMéšè—å±‚ç»´åº¦
- num_layers: LSTMå †å å±‚æ•°
- output_size: è¾“å‡ºç»´åº¦(è¿™é‡Œé¢„æµ‹æ”¶ç›˜ä»·ï¼Œæ‰€ä»¥æ˜¯1)
"""
class LSTMModel(torch.nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size  # LSTMéšè—å±‚å¤§å°
        self.num_layers = num_layers    # LSTMå±‚æ•°
        # å®šä¹‰LSTMå±‚ï¼Œbatch_first=Trueè¡¨ç¤ºè¾“å…¥æ•°æ®çš„ç¬¬ä¸€ä¸ªç»´åº¦æ˜¯batch_size
        self.lstm = torch.nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        # å®šä¹‰å…¨è¿žæŽ¥å±‚ï¼Œå°†LSTMè¾“å‡ºæ˜ å°„åˆ°é¢„æµ‹å€¼
        self.fc = torch.nn.Linear(hidden_size, output_size)

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­è¿‡ç¨‹
        
        å‚æ•°:
        x: è¾“å…¥æ•°æ®ï¼Œå½¢çŠ¶ä¸º(batch_size, seq_len, input_size)
        
        è¿”å›ž:
        é¢„æµ‹å€¼ï¼Œå½¢çŠ¶ä¸º(batch_size, output_size)
        """
        # åˆå§‹åŒ–éšè—çŠ¶æ€å’Œç»†èƒžçŠ¶æ€(å…¨é›¶åˆå§‹åŒ–)
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)
        
        # LSTMå‰å‘ä¼ æ’­
        out, _ = self.lstm(x, (h0, c0))  # outå½¢çŠ¶: (batch_size, seq_len, hidden_size)
        
        # åªå–æœ€åŽä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºï¼Œé€šè¿‡å…¨è¿žæŽ¥å±‚å¾—åˆ°é¢„æµ‹å€¼
        out = self.fc(out[:, -1, :])
        return out

"""
æ•°æ®åŠ è½½å’Œå½’ä¸€åŒ–å‡½æ•°

åŠ è½½é¢„æµ‹æ•°æ®å¹¶ä½¿ç”¨è®­ç»ƒæ•°æ®çš„å½’ä¸€åŒ–å‚æ•°è¿›è¡Œå½’ä¸€åŒ–
ç¡®ä¿é¢„æµ‹æ•°æ®çš„å½’ä¸€åŒ–æ–¹å¼ä¸Žè®­ç»ƒæ•°æ®ä¸€è‡´

å‚æ•°:
filename: é¢„æµ‹æ•°æ®æ–‡ä»¶å

è¿”å›ž:
df: åŽŸå§‹æ•°æ®DataFrame
features: å½’ä¸€åŒ–åŽçš„ç‰¹å¾æ•°æ®
target: å½’ä¸€åŒ–åŽçš„ç›®æ ‡å€¼
feature_scaler: ç‰¹å¾å½’ä¸€åŒ–å™¨
target_scaler: ç›®æ ‡å€¼å½’ä¸€åŒ–å™¨
"""
def load_data_and_scalers(filename):
    # è¯»å–é¢„æµ‹æ•°æ®
    df = pd.read_csv(os.path.join(DATA_DIR, filename))
    
    # åŠ è½½è®­ç»ƒæ—¶ä½¿ç”¨çš„å½’ä¸€åŒ–å™¨
    feature_scaler = MinMaxScaler()
    target_scaler = MinMaxScaler()
    
    # ä½¿ç”¨è®­ç»ƒæ•°æ®æ‹Ÿåˆå½’ä¸€åŒ–å™¨(ç¡®ä¿å½’ä¸€åŒ–æ–¹å¼ä¸€è‡´)
    with open(os.path.join(BASE_DIR, 'model_args.json'), 'r') as f:
        config = json.load(f)
    
    # åŠ è½½è®­ç»ƒæ•°æ®
    train_df = pd.read_csv(os.path.join(DATA_DIR, config['training']))
    train_features = train_df.drop(columns=['ts_code', 'trade_date', 'close'])
    train_target = train_df['close'].values.reshape(-1, 1)
    
    # æ‹Ÿåˆå½’ä¸€åŒ–å™¨
    feature_scaler.fit(train_features)
    target_scaler.fit(train_target)
    
    # å¯¹é¢„æµ‹æ•°æ®è¿›è¡Œç›¸åŒçš„å½’ä¸€åŒ–å¤„ç†(ç¡®ä¿åˆ—é¡ºåºä¸Žè®­ç»ƒæ—¶ä¸€è‡´)
    feature_columns = ['open', 'high', 'low', 'vol', 'amount']  # æ˜Žç¡®æŒ‡å®šç‰¹å¾åˆ—é¡ºåº
    features = df[feature_columns]
    target = df['close'].values.reshape(-1, 1)
    
    features = feature_scaler.transform(features)
    target = target_scaler.transform(target)
    
    return df, features, target, feature_scaler, target_scaler

"""
åˆ›å»ºæ»‘åŠ¨çª—å£æ•°æ®é›†

å°†æ—¶åºæ•°æ®è½¬æ¢ä¸ºé€‚åˆLSTMé¢„æµ‹çš„æ»‘åŠ¨çª—å£æ ¼å¼
ä¾‹å¦‚ï¼šç”¨å‰30å¤©çš„æ•°æ®é¢„æµ‹ç¬¬31å¤©çš„æ”¶ç›˜ä»·

å‚æ•°:
features: ç‰¹å¾æ•°æ®
target: ç›®æ ‡å€¼
window_size: æ»‘åŠ¨çª—å£å¤§å°(é»˜è®¤30å¤©)

è¿”å›ž:
X: æ»‘åŠ¨çª—å£ç‰¹å¾æ•°æ®ï¼Œå½¢çŠ¶ä¸º(n_samples, window_size, n_features)
y: å¯¹åº”çš„ç›®æ ‡å€¼
"""
def create_dataset(features, target, window_size=30):
    X, y = [], []
    # ä»Žwindow_sizeå¼€å§‹ï¼Œé¿å…è¶Šç•Œ
    for i in range(window_size, len(features)):
        # å–å‰window_sizeå¤©çš„ç‰¹å¾ä½œä¸ºè¾“å…¥
        X.append(features[i-window_size:i])
        # ç¬¬iå¤©çš„æ”¶ç›˜ä»·ä½œä¸ºç›®æ ‡
        y.append(target[i])
    return np.array(X), np.array(y)

if __name__ == '__main__':
    """
    ä¸»é¢„æµ‹æµç¨‹
    
    1. åŠ è½½é…ç½®æ–‡ä»¶
    2. åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®
    3. åˆ›å»ºæ»‘åŠ¨çª—å£æ•°æ®é›†
    4. åŠ è½½æ¨¡åž‹(å®žé™…é¡¹ç›®ä¸­åº”åŠ è½½è®­ç»ƒå¥½çš„æ¨¡åž‹)
    5. è¿›è¡Œé¢„æµ‹
    6. åå½’ä¸€åŒ–é¢„æµ‹ç»“æžœ
    7. ä¿å­˜ç»“æžœå’Œæ€§èƒ½æŒ‡æ ‡
    """
    # åŠ è½½é…ç½®æ–‡ä»¶
    with open(os.path.join(BASE_DIR, 'model_args.json'), 'r') as f:
        config = json.load(f)
    
    # åŠ è½½å¹¶é¢„å¤„ç†é¢„æµ‹æ•°æ®
    df, features, target, feature_scaler, target_scaler = load_data_and_scalers(config['predict'])
    
    # åˆ›å»ºæ»‘åŠ¨çª—å£æ•°æ®é›†
    X, y = create_dataset(features, target)
    
    # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡åž‹
    input_size = X.shape[2]  # è¾“å…¥ç‰¹å¾ç»´åº¦
    
    # ä»Žmodel_args.jsonåŠ è½½è®­ç»ƒæ—¶ä½¿ç”¨çš„æ¨¡åž‹å‚æ•°å’Œçª—å£å¤§å°
    with open(os.path.join(BASE_DIR, 'model_args.json'), 'r') as f:
        config = json.load(f)
    window_size = config['window_size']  # ä»Žé…ç½®æ–‡ä»¶è¯»å–çª—å£å¤§å°
    
    # ä½¿ç”¨ä¸Žè®­ç»ƒæ—¶ç›¸åŒçš„æ¨¡åž‹ç»“æž„
    model = LSTMModel(input_size, 
                     config['best_params']['hidden_size'], 
                     config['best_params']['num_layers'], 
                     1).to(device)
    model.load_state_dict(torch.load(os.path.join(RESULTS_DIR, 'model.pth')))  # åŠ è½½æ¨¡åž‹æƒé‡
    
    # è¿›è¡Œé¢„æµ‹
    model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—
        X_tensor = torch.FloatTensor(X).to(device)
        predictions = model(X_tensor).cpu().numpy()
    
    # åå½’ä¸€åŒ–é¢„æµ‹ç»“æžœ(è¿˜åŽŸä¸ºçœŸå®žè‚¡ä»·)
    predictions = target_scaler.inverse_transform(predictions)
    true_values = target_scaler.inverse_transform(y)
    
    # å‡†å¤‡ç»“æžœ
    # é¢„æµ‹ç»“æžœå¯¹åº”çš„æ˜¯è¾“å…¥çª—å£åŽçš„ç¬¬ä¸€ä¸ªæ—¶é—´ç‚¹
    # æ‰€ä»¥é¢„æµ‹ç»“æžœåº”è¯¥ä¸ŽåŽŸå§‹æ•°æ®çš„[window_size:]å¯¹åº”
    results = df.copy()
    results['true_close'] = np.nan  # å…ˆåˆå§‹åŒ–ä¸ºNaN
    results['predicted_close'] = np.nan
    
    # å°†é¢„æµ‹ç»“æžœå’ŒçœŸå®žå€¼å¡«å……åˆ°æ­£ç¡®ä½ç½®
    # é¢„æµ‹ç»“æžœå¯¹åº”çš„æ˜¯ç¬¬window_size+1å¤©åˆ°æœ€åŽä¸€å¤©
    # æ³¨æ„ï¼štrue_valueså’Œpredictionsçš„é•¿åº¦åº”ä¸ºlen(df)-window_size
    
    # æ£€æŸ¥é•¿åº¦æ˜¯å¦åŒ¹é…
    if len(predictions) != len(df) - window_size:
        raise ValueError(f"é¢„æµ‹ç»“æžœé•¿åº¦ä¸åŒ¹é…: predictionsé•¿åº¦={len(predictions)}, é¢„æœŸé•¿åº¦={len(df)-window_size}")
    
    # å¡«å……çœŸå®žå€¼å’Œé¢„æµ‹å€¼
    start_idx = window_size
    end_idx = len(df)  # å¡«å……åˆ°æœ€åŽä¸€å¤©
    results.iloc[start_idx:end_idx, results.columns.get_loc('true_close')] = true_values.flatten()
    results.iloc[start_idx:end_idx, results.columns.get_loc('predicted_close')] = predictions.flatten()
    
    # ç¡®ä¿å‰window_sizeå¤©æ²¡æœ‰é¢„æµ‹å€¼
    results.iloc[:window_size, results.columns.get_loc('predicted_close')] = np.nan
    
    # æŒ‰æ—¥æœŸæŽ’åºå¹¶ä¿å­˜é¢„æµ‹ç»“æžœ
    results = results.sort_values('trade_date')
    results[['trade_date', 'true_close', 'predicted_close']].to_csv(
        os.path.join(RESULTS_DIR, 'predictions.csv'), index=False)
    
    # è®¡ç®—å¹¶è®°å½•å‡æ–¹è¯¯å·®(MSE)
    mse = np.mean((results['true_close'] - results['predicted_close'])**2)
    with open(os.path.join(RESULTS_DIR, 'performance.log'), 'a') as f:
        f.write(f"Prediction MSE: {mse:.6f}\n")
    
    # æ‰“å°ç»“æžœä¿¡æ¯
    print(f"Predictions saved to {os.path.join(RESULTS_DIR, 'predictions.csv')}")
    print(f"Prediction MSE: {mse:.6f}")

    #################################################################
    # ========== ç»˜å›¾ ==========
    df_all_out = pd.read_csv(os.path.join(RESULTS_DIR, "predictions.csv"))

    df_all_out["trade_date"] = pd.to_datetime(df_all_out["trade_date"], format="%Y%m%d")
    plt.figure(figsize=(10, 4))

    if "true_close" in df_all_out.columns:
        plt.plot(df_all_out["trade_date"], df_all_out["true_close"], label="True Close")
    plt.plot(df_all_out["trade_date"], df_all_out["predicted_close"], label="Predicted Close")

    plt.xlabel("Date")
    plt.ylabel("Close Price")
    plt.title("LSTM Inference Prediction with Future Forecast")
    plt.legend()
    plt.xticks(rotation=45)
    plt.tight_layout()
    plot_path = os.path.join(RESULTS_DIR, "lstm_inference_plot.png")
    plt.savefig(plot_path)
    print(f"ðŸ“Š æŽ¨ç†å›¾ä¿å­˜è‡³ {plot_path}")


    #################################################################
    # é˜ˆå€¼ï¼šé¢„æµ‹æ¶¨è·Œå¹…å°äºŽè¯¥å€¼æ—¶å¿½ç•¥ä¿¡å·ï¼ˆå•ä½ï¼šç™¾åˆ†æ¯”ï¼‰
    threshold_pct = 0.005  # 0.5%

    # è¯»å–é¢„æµ‹æ–‡ä»¶
    pred_df = pd.read_csv(os.path.join(RESULTS_DIR, "predictions.csv"))

    # è®¡ç®—é¢„æµ‹æ¶¨è·Œå¹…
    pred_df["predicted_change"] = pred_df["predicted_close"].diff() / pred_df["predicted_close"].shift(1)
    pred_df["true_change"] = pred_df["true_close"].diff() / pred_df["true_close"].shift(1)

    # ç”Ÿæˆé¢„æµ‹è¶‹åŠ¿ä¿¡å·ï¼ˆ1 = ä¸Šæ¶¨ï¼Œ-1 = ä¸‹è·Œï¼Œ0 = æ— æ“ä½œï¼‰
    pred_df["trend_signal"] = 0
    pred_df.loc[pred_df["predicted_change"] > threshold_pct, "trend_signal"] = 1
    pred_df.loc[pred_df["predicted_change"] < -threshold_pct, "trend_signal"] = -1

    # ç”ŸæˆçœŸå®žè¶‹åŠ¿æ–¹å‘ï¼ˆç”¨äºŽéªŒè¯ï¼‰
    pred_df["true_trend"] = 0
    pred_df.loc[pred_df["true_change"] > 0, "true_trend"] = 1
    pred_df.loc[pred_df["true_change"] < 0, "true_trend"] = -1

    # è®¡ç®—è¶‹åŠ¿æ–¹å‘å‡†ç¡®çŽ‡
    valid_mask = pred_df["trend_signal"] != 0
    accuracy = (pred_df.loc[valid_mask, "trend_signal"] == pred_df.loc[valid_mask, "true_trend"]).mean()

    print(f"è¶‹åŠ¿ä¿¡å·å‡†ç¡®çŽ‡ï¼ˆè¿‡æ»¤å°æ³¢åŠ¨åŽï¼‰: {accuracy:.2%}")
    print(f"æ€»ä¿¡å·æ•°: {valid_mask.sum()} æ¡")

    # ä¿å­˜ä¿¡å·æ–‡ä»¶
    trend_path = os.path.join(RESULTS_DIR, "lstm_inference_trend_signals.csv")
    pred_df.to_csv(trend_path, index=False)
    print(f"è¶‹åŠ¿ä¿¡å·å·²ä¿å­˜åˆ° {trend_path}")

    plot_trend_signals_from_csv(trend_path, os.path.join(RESULTS_DIR, "lstm_inference_trend_signals_plot.png"))
